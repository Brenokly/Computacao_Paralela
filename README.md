üöÄ Implementa√ß√£o e Avalia√ß√£o de Programas Paralelos: C++, Kotlin e Python
Curso: Ci√™ncia da Computa√ß√£o üíª
Universidade: Ufersa - Universidade Federal Rural do Semi-√Årido üå±
Status do Projeto: Andamento - 2025
üìÑ Resumo do Projeto

Este reposit√≥rio apresenta uma an√°lise comparativa de desempenho da paraleliza√ß√£o de algoritmos cl√°ssicos ‚Äî QuickSort e Multiplica√ß√£o de Matrizes ‚Äî utilizando C++, Kotlin e Python. O objetivo foi quantificar os trade-offs de desempenho, escalabilidade e complexidade de implementa√ß√£o em arquiteturas multicore modernas.

Os resultados, obtidos atrav√©s de uma metodologia de benchmark rigorosa, demonstram que C++ e Kotlin oferecem desempenho e escalabilidade significativamente superiores para tarefas intensivas em CPU, enquanto as bibliotecas padr√£o de Python se mostraram inadequadas para este tipo de paralelismo de alta performance.
üèÜ Principais Contribui√ß√µes e Descobertas

Este estudo resultou em tr√™s contribui√ß√µes principais para o entendimento pr√°tico do paralelismo em diferentes ecossistemas:

    Identifica√ß√£o do Ponto de Inflex√£o em C++: Demonstramos quantitativamente o "ponto ideal" (maxDepth √≥timo) no QuickSort paralelo, onde o overhead de cria√ß√£o de threads com std::async supera os ganhos do paralelismo, resultando em uma degrada√ß√£o de performance.

    Demonstra√ß√£o do Risco de Deadlock em Python: Comprovamos empiricamente que o uso de concurrent.futures.ThreadPoolExecutor em algoritmos recursivos, como o QuickSort, leva a um deadlock √† medida que a profundidade da recurs√£o aumenta, travando a aplica√ß√£o.

    An√°lise Comparativa de Gargalos:

        Python (GIL): O Global Interpreter Lock torna o paralelismo com threads ineficaz para tarefas de CPU.

        Python (IPC): O custo de serializa√ß√£o de dados (IPC) na biblioteca multiprocessing anula completamente os ganhos do paralelismo em tarefas com comunica√ß√£o frequente, como a multiplica√ß√£o de matrizes.

        Kotlin (Corrotinas): O modelo de corrotinas sobre a JVM, com seu scheduler inteligente (work-stealing), mostrou-se extremamente robusto, produtivo e escal√°vel.

üõ†Ô∏è Algoritmos e Abordagens T√©cnicas

Foram implementados dois algoritmos para testar diferentes cen√°rios de paralelismo.
1. QuickSort Paralelo

Um algoritmo de "dividir para conquistar", onde a paraleliza√ß√£o intensiva de pequenas tarefas testa a efici√™ncia do gerenciador de threads e o custo de overhead.

    C++: std::async com a pol√≠tica std::launch::async para garantir a cria√ß√£o de threads nativas.

    Kotlin: Corrotinas com async e await sobre o Dispatchers.Default, um pool de threads otimizado para CPU.

    Python: concurrent.futures.ThreadPoolExecutor para avaliar o overhead e os limites do paralelismo baseado em threads sob o GIL.

// Exemplo da implementa√ß√£o em Kotlin, mostrando a cria√ß√£o de tarefas ass√≠ncronas
suspend fun parallelQuickSort(arr: IntArray, ..., depth: Int, maxDepth: Int) {
    if (low < high) {
        val pivotIndex = partition(arr, low, high)
        if (depth < maxDepth) {
            coroutineScope {
                val left = async(Dispatchers.Default) { parallelQuickSort(arr, low, pivotIndex - 1, depth + 1, maxDepth) }
                val right = async(Dispatchers.Default) { parallelQuickSort(arr, pivotIndex + 1, high, depth + 1, maxDepth) }
                left.await()
                right.await()
            }
        } else {
            // Recorre √† ordena√ß√£o sequencial para evitar overhead excessivo
            sequentialQuickSort(arr, low, pivotIndex - 1)
            sequentialQuickSort(arr, pivotIndex + 1, high)
        }
    }
}

2. Multiplica√ß√£o de Matrizes

Uma tarefa "embara√ßosamente paralela", ideal para avaliar a escalabilidade e o paralelismo bruto, contornando o GIL em Python.

    Kotlin: O c√°lculo de cada linha da matriz resultante foi distribu√≠do como uma corrotina em um pool de threads.

    Python: A biblioteca multiprocessing com Pool de processos foi utilizada para permitir a execu√ß√£o paralela real, contornando o GIL.

üî¨ Metodologia de Benchmark

Para garantir a validade estat√≠stica e a justi√ßa da compara√ß√£o, foi adotada uma metodologia rigorosa:

    Ambiente de Teste:

        CPU: AMD Ryzen 5 5600G (6 N√∫cleos, 12 Threads) @ 3.9GHz

        RAM: 16 GB DDR4

        SO: Windows 11 Pro 24H2

        Vers√µes: C++17 (MSVC v19.38), Kotlin 2.0.0 (JVM 23), Python 3.13.3

    Processo de Coleta:

        Aquecimento (Warm-up): Foram realizadas 3 execu√ß√µes iniciais cujos resultados foram descartados para mitigar efeitos de cold caches e compila√ß√£o JIT (na JVM).

        Medi√ß√µes: O tempo de execu√ß√£o de 7 medi√ß√µes independentes foi registrado para cada cen√°rio.

        An√°lise: Os resultados apresentados no artigo correspondem √† m√©dia e ao desvio padr√£o das 7 medi√ß√µes.

üìà Conclus√µes Detalhadas

    C++: Desempenho Bruto com Responsabilidade: Oferece a performance m√°xima, mas exige sintonia fina do programador para gerenciar o overhead e evitar a degrada√ß√£o de desempenho. √â a escolha ideal quando cada milissegundo conta.

    Kotlin: Robustez e Produtividade: Foi o grande destaque. Seu modelo de corrotinas se mostrou resiliente, produtivo e altamente escal√°vel, provando ser uma alternativa moderna e poderosa para computa√ß√£o de alto desempenho, abstraindo grande parte da complexidade do gerenciamento de threads.

    Python: A Ferramenta Certa para a Tarefa Certa: As bibliotecas padr√£o (threading, multiprocessing) n√£o s√£o adequadas para paralelismo de CPU de alta performance devido a gargalos arquitet√¥nicos (GIL e custo de IPC). O poder do Python para computa√ß√£o num√©rica reside em bibliotecas como NumPy e Numba, que delegam o trabalho pesado para c√≥digo C/Fortran otimizado.

üìë Refer√™ncias do Artigo

O trabalho completo se baseia e cita os seguintes artigos:

    Bulu√ß, A., & Gilbert, J. R. (2008). Challenges and Advances in Parallel Sparse Matrix-Matrix Multiplication.

    Cheng, D. R., et al. (2007). A Novel Parallel Sorting Algorithm for Contemporary Architectures.

    Hendrickson, B., et al. (1995). An Efficient Parallel Algorithm for Matrix-Vector Multiplication.

    Sanders, P., & Hansch, T. (1997). On the Efficient Implementation of Massively Parallel Quicksort.

    Schatz, M. D., et al. (2016). Parallel Matrix Multiplication: A Systematic Journey.

    Tsigas, P., & Zhang, Y. (2001). A Simple, Fast Parallel Implementation of Quicksort and its Performance Evaluation.

üë®‚Äçüéì Informa√ß√µes do Projeto

    Orientador: Prof. Paulo Henrique Lopes Silva

    Discente: Breno Klywer Olegario de Moura
